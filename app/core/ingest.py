import os
import re
from dotenv import load_dotenv 

from langchain_core.documents import Document
from langchain_pinecone import PineconeVectorStore, Pinecone
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain_openai import OpenAIEmbeddings # il modello AI per creare embeddings semantici
from pinecone import Pinecone, ServerlessSpec # SDK Pinecone
from app.core.cosine_similarity_fun import semantic_chunk_paragraphs
from app.core.merge_broke_sentence_cross_page import merge_broken_sentences
from app.core.pre_post_merge_small_parag import post_merge_semantic_small_chunks, pre_merge_small_paragraphs
# from app.core.page_overlap  import build_page_windows

# NEL FILE ingest_desc.py VIENE SPIEGATO DETTAGLIATAMENTE COSA AVVIENE IN QUESTO FILE RIGA PER RIGA

load_dotenv()

# # Inizializza Pinecone SDK

# 1Ô∏è‚É£ Crea istanza Pinecone SDK
pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))

# 2Ô∏è‚É£ Index name dal .env
index_name = os.getenv("PINECONE_INDEX_NAME") # gia creato sotto, basta crearlo 1 volta per questo e' commentato


embeddings_model = OpenAIEmbeddings(model="text-embedding-3-small")

def ingest_pdf(file_path: str, user_id: str):
    
    try:
        print('user_id = ', user_id)
    # 1Ô∏è‚É£ Carica PDF
        loader = PyPDFLoader(file_path) 
        docs = loader.load() # docs = raccolta in formato Document di tutte la pagine del pdf
          #   /-------------/
       # 2Ô∏è‚É£ Chunking
        splitter = RecursiveCharacterTextSplitter( 
           chunk_size=450, 
           chunk_overlap=50 
         )
        #   /-------------/
        
      #   Questa unisce solo quando la prima parte finisce con una lettera spezzata:
        def clean_pdf_text(text: str) -> str: # clean each pdf page

         # unisce parole spezzate da hyphen + newline
            text = re.sub(r"-\s*\n\s*", "", text)

            # parola spezzata da newline senza hyphen
            text = re.sub(
               r'(\b\w{1,3})\n(\w{2,}\b)',
               r'\1\2',
               text
            )

            return text
        
         
        # splittiamo solo se:
        # punto + newline + Maiuscola
        # + riga relativamente corta (< 80 caratteri prima del prossimo newline)
        def split_into_paragraphs(text):

            text = text.strip()
        
            # 1Ô∏è‚É£ Unisci line-break interni (ma NON dopo punto)
            text = re.sub(r'(?<!\.)\n(?=[a-z√†√®√©√¨√≤√π])', ' ', text)

            # 2Ô∏è‚É£ Trasforma linee composte solo da simboli in doppio newline
               # Esempio: "* * *"  oppure  "***"  oppure "---"
            text = re.sub(
                r'\n\s*[^A-Za-z0-9√Ä-√ñ√ò-√∂√∏-√ø]+\s*\n',
                '\n\n',
                text
            )

            # rimuovi duplicati
            # text = re.sub(r'(\b.+?\b)( \1)+', r'\1', text) puoi' provarlo piu' avanti, anche se 
            # duplicati =< 80 chars non sono un problema per embeddings o llm 
        
            # 2Ô∏è‚É£ Split principale (PDF classici)
            paragraphs = re.split(r'\n\s*\n', text)
        
            refined = []
        
            for p in paragraphs:
                p = p.strip()
        
                # 3Ô∏è‚É£ Split secondario SOLO se sembra vero nuovo paragrafo
                sub_paragraphs = re.split(
                    r'(?<=\.)\n(?=[A-Z√Ä√à√â√å√í√ô][^\.]{0,80}\n)',
                    p
                )
        
                for sp in sub_paragraphs:
                    refined.append(sp.strip())
        
            return [p for p in refined if p]



        all_chunks = []
        
        clean_paragraph_docs = [] 
        

        for page_idx, doc in enumerate(docs): 
            cleaned_pdf_text = clean_pdf_text(doc.page_content)
            paragraphs = split_into_paragraphs(cleaned_pdf_text) # non sempre riconosce i paragrafi se 
            
            paragraph_docs = [ # i documenti dei paragrafi (o blocco pagina intero se paragrafi non esistono), per ogni pagina del pdf
                Document(
                    page_content=para,
                    metadata={
                       "page": page_idx,
                       "page_start": page_idx,
                       "page_end": page_idx,
                       "paragraph_index": i
                }
               )
               for i, para in enumerate(paragraphs)
            ]

            # 3Ô∏è‚É£ merge cross-page in-line
            paragraph_docs = merge_broken_sentences(paragraph_docs) # Guarda se l‚ÄôULTIMO CHUNK 
            # finisce con punteggiatura forte e se il next inizia con una maiuscola. In tal caso unisce questi chunk. Se non
            # MAX_MERGE_LEN questo chunk puo' diventare davvero grande. (Non importa se √® fine pagina o √® met√† pagina). questa pratica
            # Aumenta logica semantica tra i chunks e crea unisce 2 chunks per un massimo di 900 chars

            # 4Ô∏è‚É£ split in small chunks con overlap e append diretto in clean_paragraph_docs
            for para_doc in paragraph_docs:
                text_len = len(para_doc.page_content)

                if text_len > 450:
                    # split con il tuo splitter (gestisce max_chars + overlap)
                    small_chunks = splitter.split_documents([para_doc])
                    for chunk in small_chunks:
                        # aggiorniamo metadata se vuoi, qui manteniamo quello originale
                        clean_paragraph_docs.append(chunk)
                else:
                    # chunk corto rimane cos√¨ com'√®. Qui non abbiamo bisogno di overlap perche' se' il chunk e <130 chars verra' merged
                    # nella fun pre_merge_small_paragraphs, se invece e' > 130 e < 450, ed simile al suo next chunk, questi verranno comunque merged  
                    # infine, se non se questo small chunk non viene merged in questi due casi, puo' venir 'merged' alla fine in "post_merge_semantic_small_chunks"
                    # cerchiamo in piu' modi logici di unire i piccoli chunks perche potrebbero creare embeddings
                    # inutili e dannosi per il nostro rag app
                    clean_paragraph_docs.append(para_doc)       
   

        clean_paragraph_docs = pre_merge_small_paragraphs( 
            clean_paragraph_docs, 
            min_chars=130,
            max_chars=600
        )

        # print('clean_paragraph_docs =',clean_paragraph_docs) # 83 chunks totali

        CROSS_PAGE_OVERLAP = 50
        # print(len('len here:',clean_paragraph_docs))
        # extra page_overlap for cross_page only
        for i in range(1, len(clean_paragraph_docs)):
            current_doc = clean_paragraph_docs[i] # il next chunk rispetto a prev_doc chunk
            prev_doc = clean_paragraph_docs[i - 1] # il chunk prima di current_doc
            # print('current page start =', current_doc.metadata.get("page_start"))
            # print('current_doc.metadata.get("page_end"):   =', current_doc.metadata.get("page_start"))
            #  Caso cross-page. Se per esempio page_start = 1 e page_end = 2
            if prev_doc.metadata.get("page_start") != current_doc.metadata.get("page_end"):
               
               # Prendiamo gli ultimi 50 chars del precedente
               prefix = prev_doc.page_content[-CROSS_PAGE_OVERLAP:]
            
               # Evitiamo doppie duplicazioni
               if not current_doc.page_content.startswith(prefix):
                
                   current_doc.page_content = prefix + " " + current_doc.page_content
               

          
        # print('pre_clean_paragraph_docs_overflow =',clean_paragraph_docs) # 83 chunks totali
        
    #    # NON AVREMO BISOGNO SI PAGE_WINDOW OVERLAP, PERCHE' qui andiamo ad unire i chunks semanticamente
    #    # simili anche se si trovano in pagine differenti. Dopodiche' raccogliamo i top 5 chunks 
    #    # piu' correlati alla query dello user (la domanda che l'utente fa' al llm dopo aver caricato il pdf)
    #    # e cosi' llm andra' a formulare riposte soddisfacenti(retrival)

        semantic_chunks = semantic_chunk_paragraphs( 
            paragraph_docs=clean_paragraph_docs, 
                
            embeddings_model=embeddings_model, 
            sim_threshold=0.65, # sweet spot per fare merge dei chunks
            max_chars = 1200 
            )
        
        # print('semantic_chunks here = ', semantic_chunks ) # 64 chunk (da 83 a 64 perche alcuni si sono uniti perche' semanticamente simili)
        
        

        semantic_chunks = post_merge_semantic_small_chunks( 
            semantic_chunks,
            min_chars=250,  
            max_chars=1200  # 1200 chars √® un buon limite per mantenere la precisione degli embeddings, ma puoi regolarlo in base alle tue esigenze specifiche
            )

        # print('post semantic_chunks here = ', semantic_chunks ) # 53 chunks

       
        # total_semantic_chars_before = sum(len(c.page_content) for c in semantic_chunks)
        # total_semantic_chars_after_final_merge = sum(len(c.page_content) for c in semantic_chunks)

        

        # print(f"Total chars in semantic chunks before final merge: {total_semantic_chars_before}")
        # print(f"Total chars in semantic chunks after final merge: {total_semantic_chars_after_final_merge}")

    #     # #  # Step 3: ulteriore split per lunghezza se necessario
        MAX_FINAL_CHUNK_CHARS = 1200 # numero massimo di chars per semantic_chunks. Oltre, gli embeddings creati perderebbero di precisione

        for chunk in semantic_chunks:
            if len(chunk.page_content) <= MAX_FINAL_CHUNK_CHARS: # nel caso post_merge_small_chunks facesse sforare
             
                 all_chunks.append(chunk)
                #  all_chunks.extend(para_chunks) # ERRATO. chunk √® un Document, non una lista.
                # extend(chunk) prova a iterare su chunk ‚Üí risultato non definito / corrotto.
            else:  # never reach this else for now because all chunks are < 1200 chars, ma √® una buona safety check comunque
                 print(f"‚ö†Ô∏è Chunk too long ({len(chunk.page_content)} chars), splitting further...")
                 para_chunks = splitter.split_documents([chunk])
                 all_chunks.extend(para_chunks)
                

    #     # #     #   debugg
        # for i, c in enumerate(all_chunks[:40]): # prendi 5 chunks
        #      print(f"CHUNK #{i}")
        #      print("PAGES:", c.metadata.get("page_start"), "‚Üí", c.metadata.get("page_end")) # i metadati definiti da noi in page?overlap.py
        #      print("PARAGRAPH:", c.metadata.get("paragraph_index"))
        #      print("CHUNK LEN:", len(c.page_content))
        #      print("TEXT:", c.page_content)
        #      print("-" * 50)
        #      if len(c.page_content) < 200:
        #          print("‚ö†Ô∏è SMALL CHUNK DETECTED")
    #     #  üßº CLEANUP: unione chunk piccoli (non piu' necessaria)
     
       # 3Ô∏è‚É£ Connetti Pinecone con namespace = user_id (storage dei pdf di ogni diverso user)
        vectorstore = PineconeVectorStore( 
           index_name=index_name, # deve eseistere. (usa index_name_large se vuoi usare il modello emb large)
           embedding=embeddings_model, # trasforma i final chunks in embeddings prima dello storage in pinecone
           namespace=user_id # inserire ordinatamente i dati nel db sotto la cartell user_id
         ) 

       # 4Ô∏è‚É£ Inserisci chunks
        vectorstore.add_documents(all_chunks) # trasforma i chunks in embeddings e inseriscili nel db (con id diverso  diverso ogni user, cosi' i dati pdf non si mescolano tra users)

        return {"message": f"{len(all_chunks)} chunks ingested for user {user_id}"}
    
    
   # üßπ Pulizia file temporanei (best practice). rimuoviamo i file gia caricati in locale(C:\Users\ale\AppData\Local\Temp\tmpxxxx.pdf) in ingest.py
   #  per non incappare in errori. 

    finally:
        if os.path.exists(file_path):
            os.remove(file_path)

